{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "modelsBuscarMejorModelo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7cP6Z1A6pQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import pickle\n",
        "import sys\n",
        "import csv"
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86hieM257dtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R_MTfYk_uNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import re as re\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from numpy import loadtxt\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtwJE9C5Bfv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ESTO NO LO EJECUTEN SI UTILIZAN UNA VERSION DE PC , YO LO HAGO PARA PODER USAR EL TRAIN DE MI DRIVE,Y USAR EL GITHUB ONLINE\n",
        "# Import PyDrive and associated libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5TzUuF2Bj4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '1BDGyvJqONz4gk2Y0BK1Ag1DA9yujd44e'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('train_uni_loca_3.csv')"
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syPUuzy1Bk9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download a file based on its file ID.\n",
        "#\n",
        "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz\n",
        "file_id = '18Ab39T1hPgTBv58oXdVk0NfBE36Fp21D'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('test_uni_loca_3.csv')\n"
      ],
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPkmzWqh_uNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cargamos los datos\n",
        "\n",
        "tweets  = pd.read_csv(\"train_uni_loca_3.csv\")\n",
        "tweets_test = pd.read_csv(\"test_uni_loca_3.csv\")"
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msQsuOldHiaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoYuHSC2Higg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEFLDjlfN_dx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tweets = pd.read_csv('train.csv')\n",
        "#tweets_test = pd.read_csv('test.csv')\n",
        "\n",
        "tweets['length']=tweets['text'].str.len()\n",
        "tweets_test['length']=tweets_test['text'].str.len()\n",
        "\n",
        "tweets['keyword'] = tweets['keyword'].str.replace('%20', ' ')\n",
        "tweets['keyword'].fillna('no keyword', inplace = True)\n",
        "\n",
        "tweets_test['keyword'] = tweets_test['keyword'].str.replace('%20', ' ')\n",
        "tweets_test['keyword'].fillna('no keyword', inplace = True)\n",
        "\n",
        "tweets = tweets.sample(frac=1,random_state=1)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51U0dvLiN_eA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['clean_text'] = tweets['text'].str.lower()\n",
        "tweets_test['clean_text'] = tweets_test['text'].str.lower()"
      ],
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvhAr5SNN_eK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def only_letters(tweet):\n",
        "    tweet = re.sub(r'http\\S*', '', tweet)\n",
        "    tweet = re.sub(r'[^a-z\\s]', '', tweet)\n",
        "    return tweet"
      ],
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8FHoNbSN_eT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['clean_text'] = tweets['clean_text'].apply(only_letters)\n",
        "tweets_test['clean_text'] = tweets_test['clean_text'].apply(only_letters)"
      ],
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQOzmYvMN_ec",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "a92caf66-6a5d-4ed0-f55d-f688e8b40fb0"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "tweets['clean_text'] = tweets['clean_text'].apply(word_tokenize)\n",
        "tweets_test['clean_text'] = tweets_test['clean_text'].apply(word_tokenize)\n",
        "from nltk.corpus import stopwords\n",
        "stop_words=set(stopwords.words(\"english\"))"
      ],
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CP3KTkw_N_eh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_stopwords(tokenized_text):\n",
        "    not_stopwords=[]\n",
        "    for w in tokenized_text:\n",
        "        if w not in stop_words:\n",
        "            not_stopwords.append(w)\n",
        "    return not_stopwords"
      ],
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gEDEdF7N_eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['clean_text'] = tweets['clean_text'].apply(filter_stopwords)\n",
        "tweets_test['clean_text'] = tweets_test['clean_text'].apply(filter_stopwords)"
      ],
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbHqHQUhN_ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdYDADw2N_e6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lemmatize_tweet(tweet):\n",
        "    lemmatized_words = []\n",
        "    for word in tweet:\n",
        "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
        "    return lemmatized_words"
      ],
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ri4o0hODN_fC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['clean_text'] = tweets['clean_text'].apply(lemmatize_tweet)\n",
        "tweets_test['clean_text'] = tweets_test['clean_text'].apply(lemmatize_tweet)\n",
        "tweets['clean_text'] = tweets['clean_text'].apply(lambda text:' '.join(text))\n",
        "tweets_test['clean_text'] = tweets_test['clean_text'].apply(lambda text:' '.join(text))\n",
        "tweets['clean_text'] = tweets['clean_text'].apply(lambda text: re.sub(r'amp | im', '', text))\n",
        "tweets_test['clean_text'] = tweets_test['clean_text'].apply(lambda text: re.sub(r'amp | im', '', text))"
      ],
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_wpSqaaN_ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_hashtags(s):\n",
        "    return list(part[1:] for part in s.split() if part.startswith('#'))"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6nONhAQN_fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['hashtags'] = tweets['text'].apply(get_hashtags)\n",
        "tweets_test['hashtags'] = tweets_test['text'].apply(get_hashtags)"
      ],
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg823vMCN_f2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hashtag_length_proportion(hashtags,length):\n",
        "    return len(''.join(hashtags))/length"
      ],
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihFjPB9mN_f5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['len_hash_over_text'] = tweets.apply(lambda data: hashtag_length_proportion(data['hashtags'],data['length']),axis=1)\n",
        "tweets_test['len_hash_over_text'] = tweets_test.apply(lambda data: hashtag_length_proportion(data['hashtags'],data['length']),axis=1)"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLGTLSvuN_f9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_mentioned_users(s):\n",
        "    return list(part[1:] for part in s.split() if part.startswith('@'))"
      ],
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "519WW7hjN_gB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['users'] = tweets['text'].apply(get_mentioned_users)\n",
        "tweets_test['users'] = tweets_test['text'].apply(get_mentioned_users)"
      ],
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zECIXmu3N_gI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def amount_hashtags(l):\n",
        "    return len(l)"
      ],
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVkfLAtZN_gO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['amount_hashtags']=tweets['hashtags'].apply(amount_hashtags)\n",
        "tweets_test['amount_hashtags']=tweets_test['hashtags'].apply(amount_hashtags)"
      ],
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jb38TOkN_gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def amount_users(l):\n",
        "    return len(l)"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH3sI6G8N_gV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def has_url(text):\n",
        "    return int('http' in text)"
      ],
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGOk7zzNN_gZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['has_url']=tweets['text'].apply(has_url)\n",
        "tweets_test['has_url']=tweets_test['text'].apply(has_url)"
      ],
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4OaewM7N_ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_url(text):\n",
        "    urls = re.findall(r'(https?://\\S+)', text)\n",
        "    return urls"
      ],
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwBgWZmcN_gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['urls']=tweets['text'].apply(find_url)\n",
        "tweets_test['urls']=tweets_test['text'].apply(find_url)\n",
        "tweets['amount_urls'] = tweets['urls'].apply(lambda x: len(x))\n",
        "tweets_test['amount_urls'] = tweets_test['urls'].apply(lambda x: len(x))"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E75G-WVMN_gn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['amount_users']=tweets['users'].apply(amount_users)\n",
        "tweets_test['amount_users']=tweets_test['users'].apply(amount_users)"
      ],
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpROGRiKN_gu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Mean encoding keyword\n",
        "tweets['keyword_encoded'] = tweets.groupby('keyword')['target'].transform('mean')"
      ],
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGBIYts3N_gx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keywords_dict = pd.Series(tweets['keyword_encoded'].values, index=tweets['keyword']).to_dict()"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUoXRs4dN_g0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets_test['keyword_encoded']= tweets_test['keyword'].map(keywords_dict)"
      ],
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DH1YnRBrN_g7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tweets['len_clean_text'] = tweets['clean_text'].str.len()\n",
        "tweets_test['len_clean_text'] = tweets_test['clean_text'].str.len()\n",
        "\n",
        "tweets['len_clean_text_over_text'] = tweets['len_clean_text']/tweets['length']\n",
        "tweets_test['len_clean_text_over_text'] = tweets_test['len_clean_text']/tweets_test['length']"
      ],
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "smv1_WO6N_hC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "2800f93f-e3ed-49b9-c117-49b29c7da63a"
      },
      "source": [
        "tweets.head()"
      ],
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>length</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>len_hash_over_text</th>\n",
              "      <th>users</th>\n",
              "      <th>amount_hashtags</th>\n",
              "      <th>has_url</th>\n",
              "      <th>urls</th>\n",
              "      <th>amount_urls</th>\n",
              "      <th>amount_users</th>\n",
              "      <th>keyword_encoded</th>\n",
              "      <th>len_clean_text</th>\n",
              "      <th>len_clean_text_over_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3228</th>\n",
              "      <td>3228</td>\n",
              "      <td>6821</td>\n",
              "      <td>loud bang</td>\n",
              "      <td>Florida</td>\n",
              "      <td>@SW_Trains strange loud impact bang noises und...</td>\n",
              "      <td>0</td>\n",
              "      <td>swtrains strange loudpact bang noise train eps...</td>\n",
              "      <td>90</td>\n",
              "      <td>[Wimbledon]</td>\n",
              "      <td>0.1</td>\n",
              "      <td>[SW_Trains]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.205882</td>\n",
              "      <td>65</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3706</th>\n",
              "      <td>3706</td>\n",
              "      <td>7840</td>\n",
              "      <td>quarantine</td>\n",
              "      <td>Diego</td>\n",
              "      <td>Loved Chicago so much that it game me Pink Eye...</td>\n",
              "      <td>0</td>\n",
              "      <td>loved chicago much game pink eye sit design qu...</td>\n",
              "      <td>99</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>59</td>\n",
              "      <td>0.595960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6957</th>\n",
              "      <td>6820</td>\n",
              "      <td>7635</td>\n",
              "      <td>pandemonium</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
              "      <td>0</td>\n",
              "      <td>pandemonium aba woman delivers baby without fa...</td>\n",
              "      <td>109</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/8j4rdwyjWu, http://t.co/9MkZPZfKL2]</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>54</td>\n",
              "      <td>0.495413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2887</th>\n",
              "      <td>2887</td>\n",
              "      <td>6074</td>\n",
              "      <td>heat wave</td>\n",
              "      <td>Maricopa, AZ</td>\n",
              "      <td>@Startide It's hotter there than Phoenix this ...</td>\n",
              "      <td>0</td>\n",
              "      <td>startide hotter phoenix week x humidity gtgt h...</td>\n",
              "      <td>94</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[Startide]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.735294</td>\n",
              "      <td>54</td>\n",
              "      <td>0.574468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7464</th>\n",
              "      <td>7432</td>\n",
              "      <td>10113</td>\n",
              "      <td>upheaval</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A look at state actions a year after Ferguson...</td>\n",
              "      <td>0</td>\n",
              "      <td>look state action year fergusons upheaval</td>\n",
              "      <td>82</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/G5ZsRU0zVQ]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>41</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0     id  ... len_clean_text len_clean_text_over_text\n",
              "3228        3228   6821  ...             65                 0.722222\n",
              "3706        3706   7840  ...             59                 0.595960\n",
              "6957        6820   7635  ...             54                 0.495413\n",
              "2887        2887   6074  ...             54                 0.574468\n",
              "7464        7432  10113  ...             41                 0.500000\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwMPxivAHinz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "outputId": "8555b842-3c79-40aa-f005-2d4172ea93e1"
      },
      "source": [
        "tweets_test.head()"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>length</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>len_hash_over_text</th>\n",
              "      <th>users</th>\n",
              "      <th>amount_hashtags</th>\n",
              "      <th>has_url</th>\n",
              "      <th>urls</th>\n",
              "      <th>amount_urls</th>\n",
              "      <th>amount_users</th>\n",
              "      <th>keyword_encoded</th>\n",
              "      <th>len_clean_text</th>\n",
              "      <th>len_clean_text_over_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Florida</td>\n",
              "      <td>Birmingham Wholesale Market is ablaze BBC News...</td>\n",
              "      <td>birmingham wholesale market ablaze bbc news fi...</td>\n",
              "      <td>120</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/irWqCEZWEU]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>82</td>\n",
              "      <td>0.683333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>47</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Niall's place | SAF 12 SQUAD |</td>\n",
              "      <td>@sunkxssedharry will you wear shorts for race ...</td>\n",
              "      <td>sunkxssedharry wear short race ablaze</td>\n",
              "      <td>54</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[sunkxssedharry]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>37</td>\n",
              "      <td>0.685185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>51</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Nigeria</td>\n",
              "      <td>#PreviouslyOnDoyinTv: Toke MakinwaÛªs marriag...</td>\n",
              "      <td>previouslyondoyintv toke makinwas marriage cri...</td>\n",
              "      <td>109</td>\n",
              "      <td>[PreviouslyOnDoyinTv:]</td>\n",
              "      <td>0.183486</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/CMghxBa2XI]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>77</td>\n",
              "      <td>0.706422</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>58</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Live On Webcam</td>\n",
              "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
              "      <td>check nsfw</td>\n",
              "      <td>114</td>\n",
              "      <td>[nsfw]</td>\n",
              "      <td>0.035088</td>\n",
              "      <td>[]</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/rOI2NSmEJJ, http://t.co/3Tj8ZjiN2...</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>10</td>\n",
              "      <td>0.087719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>60</td>\n",
              "      <td>ablaze</td>\n",
              "      <td>Los Angeles, Califnordia</td>\n",
              "      <td>PSA: IÛªm splitting my personalities.\\n\\n?? t...</td>\n",
              "      <td>psa splitting personality techie follow ablaze...</td>\n",
              "      <td>94</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>[ablaze_co, ablaze]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0.361111</td>\n",
              "      <td>69</td>\n",
              "      <td>0.734043</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  id  ... len_clean_text len_clean_text_over_text\n",
              "0           0  46  ...             82                 0.683333\n",
              "1           1  47  ...             37                 0.685185\n",
              "2           2  51  ...             77                 0.706422\n",
              "3           3  58  ...             10                 0.087719\n",
              "4           4  60  ...             69                 0.734043\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPJy3zdPHivT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpBA1Zx5Hi3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkHzGyS5Hisq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYMeOnG6Hirn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cOUG0vwHima",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSV9sAfmHzE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BCe_OY2HzNN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_set=tweets\n",
        "testing_set=tweets_test"
      ],
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvR2xwqP6pRv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_predictions(model, test_set, final_features, file_name):\n",
        "\n",
        "#\"model: el modelo entrenado\n",
        " #  test_set: registros con los features que ayudan a predecir\n",
        "#         final_features: features usados para entrenar el modelo, si se usaron todos es test_set.columns\n",
        "#        file_name: nombre del archivo donde guardar los resultados  \n",
        "# escribe un archivo con (id,target) con las predicciones del modelo y lo guarda con el nombre\n",
        "#file_name\"\"\"\n",
        "    \n",
        "\n",
        "    ids=test_set['id'].values\n",
        "    tweet_value = pd.DataFrame(ids ,columns={'id'})\n",
        "    predictions = model.predict(test_set[final_features])\n",
        "\n",
        "    tweet_value['target']=predictions\n",
        "\n",
        "    from google.colab import files\n",
        "   \n",
        " \n",
        "    tweet_value.to_csv(file_name,index=False)\n",
        "    files.download(file_name)\n",
        "    "
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiSBV7cd6pR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RedNeuronalClassifierParameterSearcher(object):\n",
        "\n",
        "    def train_and_score(self, training_set, final_features):\n",
        "\n",
        "        \"\"\"\"Devuelve un array de tuplas, donde el primer elemento de cada tupla es el score para esa combinacion de hiper-parametros,\n",
        "        se usa cross-validation\"\"\"   \n",
        "        layer_options = [5]\n",
        "        cantidadDeFeature=len(final_features)\n",
        "        actual_data = training_set[final_features]\n",
        "        labels = training_set[\"target\"]\n",
        "        scores_actuales = []\n",
        "        for layer in layer_options :\n",
        "            print (\"Ahora probando con: \" + str(layer) +\" capas de la red\")\n",
        "            #cargo datos al modelo\n",
        "            \n",
        "            clf = MLPClassifier(activation='relu',solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(layer,2),batch_size='auto', random_state=1)\n",
        "\n",
        "            scores = cross_val_score(clf, actual_data, labels, cv=40,  scoring='neg_mean_squared_error', n_jobs = -1) \n",
        "            scores_actuales.append((scores.mean(),[layer]))\n",
        "        return scores_actuales\n",
        "\n",
        "    def train_and_fit(self, training_set, final_features, hiperparameters):\n",
        "        \n",
        "        layer = hiperparameters[0]\n",
        "        #cargo datos al modelo\n",
        "        clf = MLPClassifier(activation='relu',solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(layer,2),batch_size='auto', random_state=1)\n",
        "        clf.fit(training_set[final_features], training_set[\"target\"])    \n",
        "        return clf\n"
      ],
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmTteEQt6pR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomForestRegressorParameterSearcher(object):\n",
        "\n",
        "    def train_and_score(self, training_set, final_features):\n",
        "\n",
        "        \"\"\"\"Devuelve un array de tuplas, donde el primer elemento de cada tupla es el score para esa combinacion de hiper-parametros,\n",
        "        se usa cross-validation\"\"\"   \n",
        "        sample_leaf_options = [5]\n",
        "        cantidadDeFeature=len(final_features)\n",
        "        actual_data = training_set[final_features]\n",
        "        labels = training_set[\"target\"]\n",
        "        scores_actuales = []\n",
        "        for leaf_size in sample_leaf_options :\n",
        "            print (\"Ahora probando con: \"+str(leaf_size)+\" sample_leaf\")\n",
        "            #cargo datos al modelo\n",
        "            Forest= RandomForestClassifier(n_estimators = 20, n_jobs = -1,random_state =3 ,max_features = \"auto\", min_samples_leaf = leaf_size)\n",
        "            scores = cross_val_score(Forest, actual_data, labels, cv=40,  scoring='neg_mean_squared_error', n_jobs = -1) \n",
        "            scores_actuales.append((scores.mean(),[leaf_size]))\n",
        "        return scores_actuales\n",
        "\n",
        "    def train_and_fit(self, training_set, final_features, hiperparameters):\n",
        "        \n",
        "        leaf_size = hiperparameters[0]\n",
        "        #cargo datos al modelo\n",
        "        ForestClassifier= RandomForestClassifier(n_estimators=20, oob_score = True, n_jobs = -1,random_state =50,max_features = \"auto\", min_samples_leaf = leaf_size)\n",
        "        ForestClassifier.fit(training_set[final_features], training_set[\"target\"])    \n",
        "        return ForestClassifier\n"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip9U3WEX6pSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecisionTreeRegressorParameterSearcher(object):\n",
        "\n",
        "    def train_and_score(self, training_set, final_features):\n",
        "\n",
        "        \"\"\"\"Devuelve un array de tuplas, donde el primer elemento de cada tupla es el score para esa combinacion de hiper-parametros,\n",
        "        se usa cross-validation\"\"\"    \n",
        "\n",
        "        actual_data = training_set[final_features]\n",
        "        labels = training_set[\"target\"]\n",
        "        scores_actuales = []\n",
        "        for x in range(5,40):\n",
        "            print (\"Ahora probando con: \"+str(x)+\" min_samples_split\")\n",
        "            regressor_tree = DecisionTreeClassifier(min_samples_split = x, min_samples_leaf = 15)\n",
        "            scores = cross_val_score(regressor_tree, actual_data, labels, cv=40, scoring='neg_mean_squared_error', n_jobs = -1)\n",
        "            scores_actuales.append((scores.mean(),[x]))\n",
        "        return scores_actuales\n",
        "\n",
        "    def train_and_fit(self, training_set, final_features, hiperparameters):\n",
        "        \n",
        "        min_samples = hiperparameters[0]\n",
        "        regressor_tree = DecisionTreeClassifier(min_samples_split = min_samples, min_samples_leaf = min_samples)\n",
        "        regressor_tree.fit(training_set[final_features], training_set[\"target\"])\n",
        "        return regressor_tree\n"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqqCkrSJ6pSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class KNeighborsRegressorParameterSearcher(object):\n",
        "    \n",
        "    def train_and_score(self, training_set, final_features):\n",
        "\n",
        "        \"\"\"\"Devuelve un array de tuplas, donde el primer elemento de cada tupla es el score para esa combinacion de hiper-parametros,\n",
        "        se usa cross-validation\"\"\"    \n",
        "\n",
        "        actual_data = training_set[final_features]\n",
        "        labels = training_set[\"target\"]\n",
        "        scores_actuales = []\n",
        "        for x in range(5,30):\n",
        "            print (\"Ahora probando con: \"+str(x)+\" vecinos\")\n",
        "            knn = KNeighborsClassifier(n_neighbors= x)\n",
        "            scores = cross_val_score(knn, actual_data, labels, cv=40, scoring='neg_mean_squared_error', n_jobs = -1)\n",
        "            scores_actuales.append((scores.mean(),[x]))\n",
        "        return scores_actuales\n",
        "\n",
        "    def train_and_fit(self, training_set, final_features, hiperparameters):\n",
        "        n = hiperparameters[0]\n",
        "        knn = KNeighborsClassifier(n_neighbors = n)\n",
        "        knn.fit(training_set[final_features], training_set[\"target\"])\n",
        "        return knn"
      ],
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_pnMRhl6pSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_best_model(training_set, model, final_features, cant_features, original_features):\n",
        "#training_set: el set de training, completo\n",
        "#model: el id del modelo a probar (ver arriba los ids implementados)\n",
        "#final_features: una lista, puede ser vacia o contener inicialmente algun feature que se quiere que se tenga en cuenta\n",
        "#cant_features: cantidad de features a probar, en un principio siempre es la cantidad de features - 1(id  no se prueban,)\n",
        "\n",
        "#:return: modelo fitted al training set con los mejores parametros del modelo y mejores features encontrados\n",
        "    features = list(original_features)\n",
        "    scores_previos = []\n",
        "    best_score = -float(\"inf\") #Empiezo con un error infinito\n",
        "    features_best_score = final_features #\n",
        "    best_hiper_parameters = []  \n",
        "    agregar_nuevo_feature = False #Primera iteracion la hago con el primer feature a probar\n",
        "    modelo_a_entrenar = implemented_models[model]#saca la clase a probar\n",
        "    feature_actual = 0\n",
        "    while feature_actual < cant_features:   #este ciclo recorre todas las features columnas(menos id del tweet)\n",
        "        print (\"Inicio de una nueva iteracion\")\n",
        "        print (\"Best score actual: \"+str(best_score)+\", hasta ahora las cols son: \"+str(features_best_score)+\" y los mejores hiper-parametros son: \"+str(best_hiper_parameters))\n",
        "        if agregar_nuevo_feature:            \n",
        "            final_features.append(features[feature_actual])\n",
        "\n",
        "        scores_con_features_actuales = modelo_a_entrenar.train_and_score(training_set, final_features)#al principio tendra el primer feature a probar\n",
        "        improved = False\n",
        "        for s in scores_con_features_actuales:#escore por cada hiperparemtro,por eso es un array\n",
        "            if s[0] > best_score: \n",
        "                 best_score = s[0]\n",
        "                 features_best_score = list(final_features)#ademas guarda la lista de columanas finales\n",
        "                 best_hiper_parameters = s[1]\n",
        "                 improved = True\n",
        "        if not(improved):#si no mejoro elimina esa feature,la primera no se elimnina porqeue siempre es escore es mayor a -inf\n",
        "            final_features.remove(features[feature_actual])\n",
        "      \n",
        "        feature_actual += 1\n",
        "        scores_previos.append(scores_con_features_actuales)\n",
        "       \n",
        "        agregar_nuevo_feature = True #Agrego nuevo feature a partir de la segunda iteracion\n",
        "        print (\"Fin de la ultima iteracion\")\n",
        "\n",
        "    print (\"Best score actual: \"+str(best_score)+\", hasta ahora las cols son: \"+str(features_best_score)+\" y los mejores hiper-parametros son: \"+str(best_hiper_parameters))\n",
        "    print (\"Entrenando el mejor modelo hallado,con el mejor score segun la feature y su hiperparametro\")\n",
        "    best_model = modelo_a_entrenar.train_and_fit(training_set, features_best_score, best_hiper_parameters)\n",
        "    return best_model,features_best_score#devuelve el modelo entreando y sus mejores columnas\n"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cCdcBcD6pSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#elegir uno de ellos modelos a entrenar\n",
        "implemented_models = {\"knn\":KNeighborsRegressorParameterSearcher(),\"decisiontree\":  DecisionTreeRegressorParameterSearcher(), \"randomforest\": RandomForestRegressorParameterSearcher(),\"red\":RedNeuronalClassifierParameterSearcher()}"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHRt2IBW6pSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####encontrar los mejores hiperparametros(sin buscar las mejores columnas,osea las mejores features ya vos lo pones)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qINnlLso6pSg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "468dcf2f-df25-4127-c284-e6b62cf78aa9"
      },
      "source": [
        "training_set.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>length</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>len_hash_over_text</th>\n",
              "      <th>users</th>\n",
              "      <th>amount_hashtags</th>\n",
              "      <th>has_url</th>\n",
              "      <th>urls</th>\n",
              "      <th>amount_urls</th>\n",
              "      <th>amount_users</th>\n",
              "      <th>keyword_encoded</th>\n",
              "      <th>len_clean_text</th>\n",
              "      <th>len_clean_text_over_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3228</th>\n",
              "      <td>3228</td>\n",
              "      <td>6821</td>\n",
              "      <td>loud bang</td>\n",
              "      <td>Florida</td>\n",
              "      <td>@SW_Trains strange loud impact bang noises und...</td>\n",
              "      <td>0</td>\n",
              "      <td>swtrains strange loudpact bang noise train eps...</td>\n",
              "      <td>90</td>\n",
              "      <td>[Wimbledon]</td>\n",
              "      <td>0.1</td>\n",
              "      <td>[SW_Trains]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.205882</td>\n",
              "      <td>65</td>\n",
              "      <td>0.722222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3706</th>\n",
              "      <td>3706</td>\n",
              "      <td>7840</td>\n",
              "      <td>quarantine</td>\n",
              "      <td>Diego</td>\n",
              "      <td>Loved Chicago so much that it game me Pink Eye...</td>\n",
              "      <td>0</td>\n",
              "      <td>loved chicago much game pink eye sit design qu...</td>\n",
              "      <td>99</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>59</td>\n",
              "      <td>0.595960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6957</th>\n",
              "      <td>6820</td>\n",
              "      <td>7635</td>\n",
              "      <td>pandemonium</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pandemonium In Aba As Woman Delivers Baby With...</td>\n",
              "      <td>0</td>\n",
              "      <td>pandemonium aba woman delivers baby without fa...</td>\n",
              "      <td>109</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/8j4rdwyjWu, http://t.co/9MkZPZfKL2]</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.216216</td>\n",
              "      <td>54</td>\n",
              "      <td>0.495413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2887</th>\n",
              "      <td>2887</td>\n",
              "      <td>6074</td>\n",
              "      <td>heat wave</td>\n",
              "      <td>Maricopa, AZ</td>\n",
              "      <td>@Startide It's hotter there than Phoenix this ...</td>\n",
              "      <td>0</td>\n",
              "      <td>startide hotter phoenix week x humidity gtgt h...</td>\n",
              "      <td>94</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[Startide]</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0.735294</td>\n",
              "      <td>54</td>\n",
              "      <td>0.574468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7464</th>\n",
              "      <td>7432</td>\n",
              "      <td>10113</td>\n",
              "      <td>upheaval</td>\n",
              "      <td>NaN</td>\n",
              "      <td>A look at state actions a year after Ferguson...</td>\n",
              "      <td>0</td>\n",
              "      <td>look state action year fergusons upheaval</td>\n",
              "      <td>82</td>\n",
              "      <td>[]</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[]</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>[http://t.co/G5ZsRU0zVQ]</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.157895</td>\n",
              "      <td>41</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0     id  ... len_clean_text len_clean_text_over_text\n",
              "3228        3228   6821  ...             65                 0.722222\n",
              "3706        3706   7840  ...             59                 0.595960\n",
              "6957        6820   7635  ...             54                 0.495413\n",
              "2887        2887   6074  ...             54                 0.574468\n",
              "7464        7432  10113  ...             41                 0.500000\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVgmdpOb6pSl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "features = list(training_set.columns)\n",
        "#sacamos los features que creemos indispensables\n",
        "\n",
        "features.remove('Unnamed: 0')\n",
        "features.remove('text')\n",
        "features.remove('clean_text')\n",
        "features.remove('location')\n",
        "features.remove('keyword')\n",
        "features.remove('hashtags')\n",
        "features.remove('users')\n",
        "features.remove('id')\n",
        "features.remove('urls')\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGGSutMURNJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#cambio para que en la primer posicion este nivel_lenght, ya qye ese no se evaluea\n",
        "features=['length',\n",
        " 'len_hash_over_text',\n",
        " 'amount_hashtags',\n",
        " 'has_url',\n",
        " 'amount_urls',\n",
        " 'amount_users',\n",
        " 'keyword_encoded',\n",
        " 'len_clean_text',\n",
        " \n",
        " 'len_clean_text_over_text']"
      ],
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg9fjWYzMLiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #Cantidad de features a probar con forward selection:\n",
        "cant_features = len(features)  \n",
        "#, target es lo que quiero predecir (target), ni rooms ni superficie total (estos dos features no se descartan nunca)\n",
        "final_features = [\"length\"] #fija que esta, tiene alta correlacion "
      ],
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fI5ihB3v6pSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##elegis el modelo , ejemplo knn decisiontree\n",
        "model=\"randomforest\"\n",
        "#nombre del archivo done se guardara la prediccion\n",
        "file_name=model"
      ],
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyoEKNx36pS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if model.lower() not in implemented_models:\n",
        "    print (\"No se ha implementado aun el modelo: \"+model)\n",
        "    "
      ],
      "execution_count": 251,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbbASvXh6pTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "e967a21d-b66c-4c06-bed1-c0973f43e13b"
      },
      "source": [
        "info_entrenamiento = find_best_model(training_set, model, final_features, cant_features, features)\n",
        "#te devuelve un array de dos dimentsiones, en la primera el modelo, y en la segunda las mejores columnas\n",
        "best_model = info_entrenamiento[0]\n",
        "best_features = info_entrenamiento[1]\n",
        "\n"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Inicio de una nueva iteracion\n",
            "Best score actual: -inf, hasta ahora las cols son: ['length'] y los mejores hiper-parametros son: []\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.3999848443097272, hasta ahora las cols son: ['length'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.39932626067787264, hasta ahora las cols son: ['length', 'len_hash_over_text'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.39932626067787264, hasta ahora las cols son: ['length', 'len_hash_over_text'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.3580855607605401, hasta ahora las cols son: ['length', 'len_hash_over_text', 'has_url'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.3580855607605401, hasta ahora las cols son: ['length', 'len_hash_over_text', 'has_url'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.3557136952328465, hasta ahora las cols son: ['length', 'len_hash_over_text', 'has_url', 'amount_users'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.2416967484155415, hasta ahora las cols son: ['length', 'len_hash_over_text', 'has_url', 'amount_users', 'keyword_encoded'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Inicio de una nueva iteracion\n",
            "Best score actual: -0.23526935794984843, hasta ahora las cols son: ['length', 'len_hash_over_text', 'has_url', 'amount_users', 'keyword_encoded', 'len_clean_text'] y los mejores hiper-parametros son: [5]\n",
            "Ahora probando con: 5 sample_leaf\n",
            "Fin de la ultima iteracion\n",
            "Best score actual: -0.23526935794984843, hasta ahora las cols son: ['length', 'len_hash_over_text', 'has_url', 'amount_users', 'keyword_encoded', 'len_clean_text'] y los mejores hiper-parametros son: [5]\n",
            "Entrenando el mejor modelo hallado,con el mejor score segun la feature y su hiperparametro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqE_pAAuf9ca",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9428a779-2582-4fc1-b9ce-c56d268e7945"
      },
      "source": [
        "best_features"
      ],
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['length',\n",
              " 'len_hash_over_text',\n",
              " 'has_url',\n",
              " 'amount_users',\n",
              " 'keyword_encoded',\n",
              " 'len_clean_text']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 253
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo9-C2hDTyeV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "adaddc6e-5c34-4906-850f-844b43240ac0"
      },
      "source": [
        "make_predictions(best_model, testing_set, best_features, file_name+\".csv\")\n",
        "pickle.dump(best_model, open(file_name+\".sav\", 'wb'))"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_a88ee27a-826e-444d-bade-652db2c1768f\", \"randomforest.csv\", 22746)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ycm7a4BX6pTJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}